# 模型训练测试方案评估报告

## 概述

本报告针对厂商芯片在模型训练场景下的性能表现，从**精度测试**、**确定性测试**、**内存管理**和**训练效率**四个核心维度展开全面评估，旨在验证厂商芯片与传统 GPU 在训练过程中的兼容性、稳定性及性能优势，为大规模模型训练的硬件选型提供数据支撑。

## 一、精度测试

精度测试是模型训练的核心验证环节，确保厂商芯片在计算过程中不会因硬件差异导致模型精度损失，主要分为**算子精度测试**和**模型 Loss 曲线精度测试**两类。

### 1.1 算子精度测试

针对厂商芯片特有的非线性指令及关键训练算子，与 GPU 进行点对点精度对比，结果如下：



*   **非线性指令**：厂商芯片的非线性计算指令（如激活函数、归一化操作等）精度与 GPU 完全相当，无系统性偏差。

*   **Flash Attention（FA）**：


    *   前向计算：输出结果与 GPU 逐元素对比，误差在可接受范围内（小于 1e-5），符合精度预期。

    *   反向计算：梯度传递过程中的精度损失与 GPU 一致，无额外误差引入，确保模型训练的梯度正确性。

### 1.2 模型 Loss 曲线精度测试

在不同卡数（16 卡、32 卡、240 卡）和不同模型规模（21B、24B、66B）下，对比厂商芯片与 GPU 的训练 Loss 曲线，核心结论如下：



*   **Loss 收敛趋势**：厂商芯片在各配置下的 Loss 曲线与 GPU 完全重合，收敛速度一致，无精度偏移。

*   **误差控制**：选取训练过程中的 50 个连续 step 计算平均 Loss 误差，结果显示厂商芯片与 GPU 的平均误差**均小于 0.2%**，满足大规模模型训练的精度要求。

## 二、确定性测试

确定性是保证模型训练可复现性的关键，需验证算子级和多机通信级的结果一致性，测试覆盖核心算子及通信操作：

### 2.1 算子确定性

对训练过程中常用的随机及计算类算子进行多次重复执行，结果均实现**bit 级对齐**：


*   随机算子：rand（随机数生成）、dropout（随机失活）在相同种子（seed）下，输出结果与 GPU 完全一致，无随机性偏差。

*   计算算子：Flash Attention（前向 / 后向）、矩阵乘（GEMM）等算子的计算结果在多次执行中完全相同，无浮点精度波动。

### 2.2 多机通信确定性

针对分布式训练中的核心通信操作 all-reduce，在多机多卡（如 16 机 128 卡、30 机 240 卡）环境下测试：



*   相同输入数据下，多机 all-reduce 的聚合结果与 GPU 集群实现**bit 级对齐**，无通信过程中的数据损坏或精度损失，确保分布式训练的一致性。

## 三、内存管理

内存管理能力直接影响模型训练的稳定性和最大支持模型规模，从内存占用和碎片化处理两方面评估：

### 3.1 内存占用

在相同模型（21B/24B/66B）和相同 batch size 配置下，对比厂商芯片与 GPU 的内存使用情况：



*   厂商芯片的内存占用量**略低于 GPU（波动范围 ±0.2%）**，内存利用效率更高，可支持更大 batch size 或更大规模模型的训练。

*   长期训练过程中（持续 1000+ step），内存占用保持稳定，无内存泄漏现象，确保训练任务长时间运行的可靠性。

### 3.2 碎片化处理

通过设计**变长输入序列**（如文本序列长度在 64\~2048 之间动态变化），模拟真实训练中的内存碎片化场景：



*   厂商芯片的内存碎片化处理能力与 GPU 相当，在变长序列训练中，内存分配 / 释放效率无明显下降，无因碎片化导致的 OOM（内存溢出）问题，确保复杂场景下的训练稳定性。

## 四、训练效率

训练效率以**MFU（Model Flops Utilization，模型计算效率）** 为核心指标，基于 240 卡实测数据，预估更大规模集群（960 卡、1200 卡）的利用率：


*   **960 卡集群**：预估 MFU 为**XX%\~XX%**（较 240 卡下降幅度小于 5%），主要因通信链路增加导致的轻微 overhead，整体效率保持稳定。

*   **1200 卡集群**：预估 MFU 为**XX%\~XX%**（较 240 卡下降幅度小于 8%），在超大规模集群下，仍能维持较高的计算效率，满足万亿级模型训练的需求。


